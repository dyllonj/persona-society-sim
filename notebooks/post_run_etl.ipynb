{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Post-run telemetry ETL\n",
        "\n",
        "Merge action/message cognitive traces with MetricTracker exports to audit steering coverage by trait and alpha bucket. Configure the paths below to point at a simulation dump (Parquet folders) and the associated `metrics/` tracker outputs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Paths\n",
        "\n",
        "Set `DUMP_DIR` to the folder containing `actions/`, `messages/`, and `metrics_snapshots/` Parquet dumps. If the `metrics/` directory lives elsewhere (e.g., a shared tracker output cache), override `METRIC_DIR` as well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "import hashlib\n",
        "import json\n",
        "from pathlib import Path\n",
        "from typing import Dict, Iterable, Optional, Tuple\n",
        "\n",
        "import pandas as pd\n",
        "import pyarrow.parquet as pq"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Root folders for a single run.\n",
        "DUMP_DIR = Path(\"artifacts/demo_dump\")\n",
        "METRIC_DIR = DUMP_DIR / \"metrics\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Loaders\n",
        "\n",
        "Helper utilities to load Parquet shards, flatten nested fields, and pick up the MetricTracker JSONL summary (plus its companion Parquet aggregates if present)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def _read_parquet_dir(directory: Path) -> pd.DataFrame:\n",
        "    '''Load every Parquet file in a directory into a single DataFrame.'''\n",
        "    frames = []\n",
        "    if not directory.exists():\n",
        "        return pd.DataFrame()\n",
        "    for file_path in sorted(directory.glob('*.parquet')):\n",
        "        frames.append(pq.read_table(file_path).to_pandas())\n",
        "    if not frames:\n",
        "        return pd.DataFrame()\n",
        "    return pd.concat(frames, ignore_index=True)\n",
        "\n",
        "\n",
        "def load_action_logs(dump_dir: Path) -> pd.DataFrame:\n",
        "    raw = _read_parquet_dir(dump_dir / 'actions')\n",
        "    if raw.empty:\n",
        "        return raw\n",
        "    df = raw.copy()\n",
        "    # Preserve the raw JSON blobs for joins while also flattening.\n",
        "    df['info_raw'] = df.get('info')\n",
        "    df['plan_metadata_raw'] = df.get('plan_metadata')\n",
        "    info_df = pd.json_normalize(df.pop('info'), sep='.').add_prefix('info.')\n",
        "    plan_df = pd.json_normalize(df.pop('plan_metadata'), sep='.').add_prefix('plan_metadata.')\n",
        "    return pd.concat([df.reset_index(drop=True), info_df, plan_df], axis=1)\n",
        "\n",
        "\n",
        "def load_message_logs(dump_dir: Path) -> pd.DataFrame:\n",
        "    return _read_parquet_dir(dump_dir / 'messages')\n",
        "\n",
        "\n",
        "def load_metric_snapshots(dump_dir: Path) -> pd.DataFrame:\n",
        "    return _read_parquet_dir(dump_dir / 'metrics_snapshots')\n",
        "\n",
        "\n",
        "def load_metric_tracker(metrics_dir: Path):\n",
        "    '''Return (agent_metrics, trait_metrics, summary_dict).'''\n",
        "    if not metrics_dir.exists():\n",
        "        return pd.DataFrame(), None, None\n",
        "    jsonl_candidates = sorted(metrics_dir.glob('run_*.jsonl'))\n",
        "    summary = None\n",
        "    if jsonl_candidates:\n",
        "        with jsonl_candidates[-1].open('r', encoding='utf-8') as handle:\n",
        "            first_line = handle.readline().strip()\n",
        "            if first_line:\n",
        "                try:\n",
        "                    payload = json.loads(first_line)\n",
        "                    summary = payload.get('summary')\n",
        "                except json.JSONDecodeError:\n",
        "                    summary = None\n",
        "    agent_parquet = None\n",
        "    trait_parquet = None\n",
        "    parquet_agents = metrics_dir.glob('run_*_agents.parquet')\n",
        "    parquet_traits = metrics_dir.glob('run_*_trait_aggregates.parquet')\n",
        "    for path in parquet_agents:\n",
        "        agent_parquet = pd.read_parquet(path)\n",
        "    for path in parquet_traits:\n",
        "        trait_parquet = pd.read_parquet(path)\n",
        "    return agent_parquet if agent_parquet is not None else pd.DataFrame(), trait_parquet, summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cognitive-trace joins\n",
        "\n",
        "Line up action and message logs so every decision is paired with the steering snapshot that actually hit the model. We prefer `prompt_hash` when present and fall back to a deterministic signature of the plan metadata to disambiguate multi-action ticks."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "COOP_ACTIONS = {'talk', 'work', 'gift', 'research', 'cite', 'submit_report', 'scan'}\n",
        "\n",
        "\n",
        "def _normalize_snapshot(snapshot: object) -> Dict[str, float]:\n",
        "    if isinstance(snapshot, str):\n",
        "        try:\n",
        "            snapshot = json.loads(snapshot)\n",
        "        except json.JSONDecodeError:\n",
        "            return {}\n",
        "    return snapshot or {}\n",
        "\n",
        "\n",
        "def _plan_signature(plan_metadata: object) -> Optional[str]:\n",
        "    if isinstance(plan_metadata, dict):\n",
        "        try:\n",
        "            return json.dumps(plan_metadata, sort_keys=True)\n",
        "        except TypeError:\n",
        "            return None\n",
        "    return None\n",
        "\n",
        "\n",
        "def align_actions_messages(actions: pd.DataFrame, messages: pd.DataFrame) -> pd.DataFrame:\n",
        "    if actions.empty or messages.empty:\n",
        "        return pd.DataFrame()\n",
        "    work = actions.copy()\n",
        "    work['plan_signature'] = work.get('plan_metadata_raw', pd.Series()).apply(_plan_signature)\n",
        "    work['join_basis'] = work.apply(\n",
        "        lambda row: 'prompt_hash'\n",
        "        if pd.notna(row.get('prompt_hash'))\n",
        "        else ('plan_metadata' if row.get('plan_signature') else 'tick_agent'),\n",
        "        axis=1,\n",
        "    )\n",
        "    merged = pd.merge(\n",
        "        work,\n",
        "        messages,\n",
        "        left_on=['run_id', 'tick', 'agent_id'],\n",
        "        right_on=['run_id', 'tick', 'from_agent'],\n",
        "        suffixes=('_action', '_msg'),\n",
        "        how='inner',\n",
        "    )\n",
        "    merged['steering_match'] = merged.apply(\n",
        "        lambda row: _normalize_snapshot(row.get('info.steering_snapshot'))\n",
        "        == _normalize_snapshot(row.get('steering_snapshot')),\n",
        "        axis=1,\n",
        "    )\n",
        "    return merged"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Metric rollups\n",
        "\n",
        "Macro metrics come from `metrics_snapshots/` (trait cohorts already baked in). We weight each tick by the share of actions contributed by a given alpha bucket to capture steering intensity alongside the trait band views."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def trait_band_macro(metrics_df: pd.DataFrame) -> pd.DataFrame:\n",
        "    if metrics_df.empty:\n",
        "        return pd.DataFrame()\n",
        "    fields = ['cooperation_rate', 'gini_wealth', 'polarization_modularity']\n",
        "    return (\n",
        "        metrics_df.dropna(subset=['trait_key'])\n",
        "        .groupby('trait_key')[fields]\n",
        "        .mean()\n",
        "        .reset_index()\n",
        "    )\n",
        "\n",
        "\n",
        "def alpha_bucket_macro(actions_df: pd.DataFrame, metrics_df: pd.DataFrame) -> pd.DataFrame:\n",
        "    if actions_df.empty or metrics_df.empty:\n",
        "        return pd.DataFrame()\n",
        "    action_buckets = (\n",
        "        actions_df.dropna(subset=['info.trait_key', 'info.alpha_bucket'])\n",
        "        .groupby(['tick', 'info.trait_key', 'info.alpha_bucket'])\n",
        "        .size()\n",
        "        .reset_index(name='action_count')\n",
        "    )\n",
        "    action_buckets['tick_total'] = action_buckets.groupby(['tick', 'info.trait_key'])['action_count'].transform('sum')\n",
        "    action_buckets['weight'] = action_buckets['action_count'] / action_buckets['tick_total']\n",
        "\n",
        "    weighted = metrics_df.merge(\n",
        "        action_buckets,\n",
        "        left_on=['tick', 'trait_key'],\n",
        "        right_on=['tick', 'info.trait_key'],\n",
        "        how='inner',\n",
        "    )\n",
        "    if weighted.empty:\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    for col in ['cooperation_rate', 'gini_wealth', 'polarization_modularity']:\n",
        "        weighted[f'weighted::{col}'] = weighted[col] * weighted['weight']\n",
        "\n",
        "    aggregated = (\n",
        "        weighted.groupby(['info.alpha_bucket', 'trait_key'])[\n",
        "            ['weighted::cooperation_rate', 'weighted::gini_wealth', 'weighted::polarization_modularity', 'weight']\n",
        "        ]\n",
        "        .sum()\n",
        "        .reset_index()\n",
        "    )\n",
        "    aggregated.rename(columns={'info.alpha_bucket': 'alpha_bucket'}, inplace=True)\n",
        "    aggregated['cooperation_rate'] = aggregated['weighted::cooperation_rate'] / aggregated['weight']\n",
        "    aggregated['gini_wealth'] = aggregated['weighted::gini_wealth'] / aggregated['weight']\n",
        "    aggregated['polarization_modularity'] = aggregated['weighted::polarization_modularity'] / aggregated['weight']\n",
        "    return aggregated[['alpha_bucket', 'trait_key', 'cooperation_rate', 'gini_wealth', 'polarization_modularity']]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run the ETL\n",
        "\n",
        "Execute this block after pointing the paths at a real run. The outputs include\n",
        "\n",
        "- the per-action steering alignment join,\n",
        "- trait band macro metrics, and\n",
        "- alpha-bucket-weighted macro metrics."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "actions = load_action_logs(DUMP_DIR)\n",
        "messages = load_message_logs(DUMP_DIR)\n",
        "metrics_snapshots = load_metric_snapshots(DUMP_DIR)\n",
        "agent_metrics, trait_metrics, tracker_summary = load_metric_tracker(METRIC_DIR)\n",
        "\n",
        "print(f'actions: {len(actions)}, messages: {len(messages)}, metrics_snapshots: {len(metrics_snapshots)}')\n",
        "if tracker_summary:\n",
        "    print('metric tracker summary keys:', sorted(tracker_summary.keys()))\n",
        "\n",
        "joined = align_actions_messages(actions, messages)\n",
        "print(f'aligned rows: {len(joined)}, steering agreement: {joined[\"steering_match\"].mean() if not joined.empty else 0:.3f}')\n",
        "\n",
        "band_view = trait_band_macro(metrics_snapshots)\n",
        "alpha_view = alpha_bucket_macro(actions, metrics_snapshots)\n",
        "\n",
        "band_view"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "alpha_view"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}