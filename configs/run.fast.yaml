run_id: "fast_test"
git_commit: "local"
model_name: "meta-llama/Llama-3.1-8B-Instruct"
population: 32
steps: 200
seed: 7
steering:
  strength: 1.0
  coefficients:
    E: 0.8
    A: 0.5
    C: 0.6
    O: 0.0
    N: 0.0
  vector_norm:
    E: 1.0
    A: 1.0
    C: 1.0
    O: 1.0
    N: 1.0
  metadata_files:
    personas: configs/personas.bigfive.yaml
    vectors: configs/steering.layers.yaml
scenario: "market_voting_baseline"
logging:
  db_url: "sqlite:///./storage/sim_fast_test.db"
  parquet_dir: "./storage/dumps/fast_test"
safety:
  alpha_clip: 1.0
  toxicity_threshold: 0.4
  governor_backoff: 0.2
inference:
  temperature: 0.7
  top_p: 0.9
  max_new_tokens: 48  # Reduced from 128 to 48 (3x faster)
optimization:
  reflect_every_n_ticks: 5  # Only reflect every 5 ticks instead of every tick
  use_quantization: true     # Enable 4-bit quantization
  batch_size: 8              # Batch 8 agents at once (future feature)
objectives:
  enabled: true
